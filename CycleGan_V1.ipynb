{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "CycleGan_V1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BraianBGit/DeepLearning2021/blob/main/CycleGan_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAQb0XDnp0Jy",
        "outputId": "11e53a1d-72d6-4c62-d751-d240cf2c9511"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2MzUWaNqi-0",
        "outputId": "1e0cea64-c310-4f6c-9f04-43d1281db687"
      },
      "source": [
        "!pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKb1XAfz950u"
      },
      "source": [
        "**Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRRBBO9M9zkT"
      },
      "source": [
        "import os, random, json, PIL, shutil, re, imageio, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from PIL import ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as L\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import Model, losses, optimizers\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "\n",
        "def seed_everything(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    \n",
        "SEED = 0\n",
        "seed_everything(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyJv6eY098pf"
      },
      "source": [
        "**TPU configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhgFeZlm94db",
        "outputId": "d7bbfb9d-2420-4b9b-b98a-130aebc5c541"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Device:', tpu.master())\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
        "\n",
        "    \n",
        "print(tf.__version__)\n",
        "REPLICAS = strategy.num_replicas_in_sync\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "print(f'REPLICAS: {REPLICAS}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: grpc://10.41.195.210:8470\n",
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.41.195.210:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.41.195.210:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of replicas: 8\n",
            "2.6.0\n",
            "REPLICAS: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i72SF6RF9__I"
      },
      "source": [
        "**Model parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtLrZeBq-C_X"
      },
      "source": [
        "HEIGHT = 256\n",
        "WIDTH = 256\n",
        "HEIGHT_RESIZE = 128\n",
        "WIDTH_RESIZE = 128\n",
        "CHANNELS = 3\n",
        "BATCH_SIZE = 58\n",
        "EPOCHS = 200\n",
        "TRANSFORMER_BLOCKS = 6\n",
        "GENERATOR_LR = 2e-4\n",
        "DISCRIMINATOR_LR = 2e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQlyyGZ8-0AR"
      },
      "source": [
        "**Load data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkJOnpWi-Ef-",
        "outputId": "4467ba4f-0dee-46f8-a166-800d017bded0"
      },
      "source": [
        "PHOTO_FILENAMES = tf.io.gfile.glob(str('/content/drive/MyDrive/DeepLearningTPs/dataset/training_real_resized/*.jpg'))\n",
        "VINTAGE_FILENAMES = tf.io.gfile.glob(str('/content/drive/MyDrive/DeepLearningTPs/dataset/vintage_portrait_resized/*.jpg'))\n",
        "\n",
        "n_photo_samples = len(PHOTO_FILENAMES)\n",
        "n_vintage_samples = len(VINTAGE_FILENAMES)\n",
        "\n",
        "print(f'Photo image files: {n_photo_samples}')\n",
        "print(f'Vintage image files: {n_vintage_samples}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Photo image files: 1081\n",
            "Vintage image files: 1459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0IPEpXv_ACK"
      },
      "source": [
        "**Auxiliar functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqOeaJIT_B01"
      },
      "source": [
        "def normalize_img(img):\n",
        "    img = tf.cast(img, dtype=tf.float32)\n",
        "    # Map values in the range [-1, 1]\n",
        "    return (img / 127.5) - 1.0\n",
        "\n",
        "def decode_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n",
        "    image = tf.reshape(image, [HEIGHT, WIDTH, CHANNELS])\n",
        "    return image\n",
        "\n",
        "def read_tfrecord(example):\n",
        "    tfrecord_format = {\n",
        "        'image':      tf.io.FixedLenFeature([], tf.string)\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
        "    image = decode_image(example['image'])\n",
        "    return image\n",
        "\n",
        "def load_dataset(filenames):\n",
        "    tensors = []\n",
        "  \n",
        "    for filename in filenames:\n",
        "      with open(filename, 'rb') as local_file:\n",
        "        image = local_file.read()\n",
        "      tensor = decode_image(image) \n",
        "      tensors.append(tensor)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(tensors)\n",
        "    # dataset = dataset.map(decode_image, num_parallel_calls=AUTO)\n",
        "    return dataset\n",
        "\n",
        "def get_dataset(filenames, augment=None, repeat=True, shuffle=True, batch_size=1):\n",
        "    dataset = load_dataset(filenames)\n",
        "\n",
        "    if augment:\n",
        "        dataset = dataset.map(augment, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.map(normalize_img, num_parallel_calls=AUTO)\n",
        "    if repeat:\n",
        "        dataset = dataset.repeat()\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(512)\n",
        "        \n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def display_samples(ds, n_samples):\n",
        "    ds_iter = iter(ds)\n",
        "    for n_sample in range(n_samples):\n",
        "        example_sample = next(ds_iter)\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(example_sample[0] * 0.5 + 0.5)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        \n",
        "def display_generated_samples(ds, model, n_samples):\n",
        "    ds_iter = iter(ds)\n",
        "    for n_sample in range(n_samples):\n",
        "        example_sample = next(ds_iter)\n",
        "        generated_sample = model.predict(example_sample)\n",
        "        \n",
        "        f = plt.figure(figsize=(12, 12))\n",
        "        \n",
        "        plt.subplot(121)\n",
        "        plt.title('Input image')\n",
        "        plt.imshow(example_sample[0] * 0.5 + 0.5)\n",
        "        plt.axis('off')\n",
        "        \n",
        "        plt.subplot(122)\n",
        "        plt.title('Generated image')\n",
        "        plt.imshow(generated_sample[0] * 0.5 + 0.5)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        \n",
        "def evaluate_cycle(ds, generator_a, generator_b, n_samples=1):\n",
        "    fig, axes = plt.subplots(n_samples, 3, figsize=(22, (n_samples*6)))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    ds_iter = iter(ds)\n",
        "    for n_sample in range(n_samples):\n",
        "        idx = n_sample*3\n",
        "        example_sample = next(ds_iter)\n",
        "        generated_a_sample = generator_a.predict(example_sample)\n",
        "        generated_b_sample = generator_b.predict(generated_a_sample)\n",
        "        \n",
        "        axes[idx].set_title('Input image', fontsize=18)\n",
        "        axes[idx].imshow(example_sample[0] * 0.5 + 0.5)\n",
        "        axes[idx].axis('off')\n",
        "        \n",
        "        axes[idx+1].set_title('Generated image', fontsize=18)\n",
        "        axes[idx+1].imshow(generated_a_sample[0] * 0.5 + 0.5)\n",
        "        axes[idx+1].axis('off')\n",
        "        \n",
        "        axes[idx+2].set_title('Cycled image', fontsize=18)\n",
        "        axes[idx+2].imshow(generated_b_sample[0] * 0.5 + 0.5)\n",
        "        axes[idx+2].axis('off')\n",
        "        \n",
        "    plt.show()\n",
        "\n",
        "def create_gif(images_path, gif_path):\n",
        "    images = []\n",
        "    filenames = glob.glob(images_path)\n",
        "    filenames.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
        "    for epoch, filename in enumerate(filenames):\n",
        "        img = PIL.ImageDraw.Image.open(filename)\n",
        "        ImageDraw.Draw(img).text((0, 0),  # Coordinates\n",
        "                                 f'Epoch {epoch+1}')\n",
        "        images.append(np.array(img))\n",
        "    imageio.mimsave(gif_path, images, fps=2) # Save gif\n",
        "        \n",
        "def predict_and_save(input_ds, generator_model, output_path):\n",
        "    i = 1\n",
        "    for img in input_ds:\n",
        "        prediction = generator_model(img, training=False)[0].numpy() # make predition\n",
        "        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)   # re-scale\n",
        "        im = PIL.Image.fromarray(prediction)\n",
        "        im.save(f'{output_path}{str(i)}.jpg')\n",
        "        i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIk-PeFS_Qin"
      },
      "source": [
        "**Auxiliar functions (model)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSuR5np7_T57"
      },
      "source": [
        "conv_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02)\n",
        "gamma_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "    \n",
        "def encoder_block(input_layer, filters, size=3, strides=2, apply_instancenorm=True, activation=L.ReLU(), name='block_x'):\n",
        "    block = L.Conv2D(filters, size, \n",
        "                     strides=strides, \n",
        "                     padding='same', \n",
        "                     use_bias=False, \n",
        "                     kernel_initializer=conv_initializer, \n",
        "                     name=f'encoder_{name}')(input_layer)\n",
        "\n",
        "    if apply_instancenorm:\n",
        "        block = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(block)\n",
        "        \n",
        "    block = activation(block)\n",
        "\n",
        "    return block\n",
        "\n",
        "def transformer_block(input_layer, size=3, strides=1, name='block_x'):\n",
        "    filters = input_layer.shape[-1]\n",
        "    \n",
        "    block = L.Conv2D(filters, size, strides=strides, padding='same', use_bias=False, \n",
        "                     kernel_initializer=conv_initializer, name=f'transformer_{name}_1')(input_layer)\n",
        "#     block = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(block)\n",
        "    block = L.ReLU()(block)\n",
        "    \n",
        "    block = L.Conv2D(filters, size, strides=strides, padding='same', use_bias=False, \n",
        "                     kernel_initializer=conv_initializer, name=f'transformer_{name}_2')(block)\n",
        "#     block = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(block)\n",
        "    \n",
        "    block = L.Add()([block, input_layer])\n",
        "\n",
        "    return block\n",
        "\n",
        "def decoder_block(input_layer, filters, size=3, strides=2, apply_instancenorm=True, name='block_x'):\n",
        "    block = L.Conv2DTranspose(filters, size, \n",
        "                              strides=strides, \n",
        "                              padding='same', \n",
        "                              use_bias=False, \n",
        "                              kernel_initializer=conv_initializer, \n",
        "                              name=f'decoder_{name}')(input_layer)\n",
        "\n",
        "    if apply_instancenorm:\n",
        "        block = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(block)\n",
        "\n",
        "    block = L.ReLU()(block)\n",
        "    \n",
        "    return block\n",
        "\n",
        "# Resized convolution\n",
        "def decoder_rc_block(input_layer, filters, size=3, strides=1, apply_instancenorm=True, name='block_x'):\n",
        "    block = tf.image.resize(images=input_layer, method='bilinear', \n",
        "                            size=(input_layer.shape[1]*2, input_layer.shape[2]*2))\n",
        "    \n",
        "#     block = tf.pad(block, [[0, 0], [1, 1], [1, 1], [0, 0]], \"SYMMETRIC\") # Works only with GPU\n",
        "#     block = L.Conv2D(filters, size, strides=strides, padding='valid', use_bias=False, # Works only with GPU\n",
        "    block = L.Conv2D(filters, size, \n",
        "                     strides=strides, \n",
        "                     padding='same', \n",
        "                     use_bias=False, \n",
        "                     kernel_initializer=conv_initializer, \n",
        "                     name=f'decoder_{name}')(block)\n",
        "\n",
        "    if apply_instancenorm:\n",
        "        block = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(block)\n",
        "\n",
        "    block = L.ReLU()(block)\n",
        "    \n",
        "    return block"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oClDS87y_XEL"
      },
      "source": [
        "**Generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3IxZyNq_aqD",
        "outputId": "458988a0-b801-43da-a399-407cc2955eea"
      },
      "source": [
        "def generator_fn(height=HEIGHT, width=WIDTH, channels=CHANNELS, transformer_blocks=TRANSFORMER_BLOCKS):\n",
        "    OUTPUT_CHANNELS = 3\n",
        "    inputs = L.Input(shape=[height, width, channels], name='input_image')\n",
        "\n",
        "    # Encoder\n",
        "    enc_1 = encoder_block(inputs, 64,  7, 1, apply_instancenorm=False, activation=L.ReLU(), name='block_1') # (bs, 256, 256, 64)\n",
        "    enc_2 = encoder_block(enc_1, 128, 3, 2, apply_instancenorm=True, activation=L.ReLU(), name='block_2')   # (bs, 128, 128, 128)\n",
        "    enc_3 = encoder_block(enc_2, 256, 3, 2, apply_instancenorm=True, activation=L.ReLU(), name='block_3')   # (bs, 64, 64, 256)\n",
        "    \n",
        "    # Transformer\n",
        "    x = enc_3\n",
        "    for n in range(transformer_blocks):\n",
        "        x = transformer_block(x, 3, 1, name=f'block_{n+1}') # (bs, 64, 64, 256)\n",
        "\n",
        "    # Decoder\n",
        "    x_skip = L.Concatenate(name='enc_dec_skip_1')([x, enc_3]) # encoder - decoder skip connection\n",
        "    \n",
        "    dec_1 = decoder_block(x_skip, 128, 3, 2, apply_instancenorm=True, name='block_1') # (bs, 128, 128, 128)\n",
        "    x_skip = L.Concatenate(name='enc_dec_skip_2')([dec_1, enc_2]) # encoder - decoder skip connection\n",
        "    \n",
        "    dec_2 = decoder_block(x_skip, 64,  3, 2, apply_instancenorm=True, name='block_2') # (bs, 256, 256, 64)\n",
        "    x_skip = L.Concatenate(name='enc_dec_skip_3')([dec_2, enc_1]) # encoder - decoder skip connection\n",
        "\n",
        "    outputs = last = L.Conv2D(OUTPUT_CHANNELS, 7, \n",
        "                              strides=1, padding='same', \n",
        "                              kernel_initializer=conv_initializer, \n",
        "                              use_bias=False, \n",
        "                              activation='tanh', \n",
        "                              name='decoder_output_block')(x_skip) # (bs, 256, 256, 3)\n",
        "\n",
        "    generator = Model(inputs, outputs)\n",
        "    \n",
        "    return generator\n",
        "\n",
        "sample_generator = generator_fn()\n",
        "sample_generator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_image (InputLayer)        [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_block_1 (Conv2D)        (None, 256, 256, 64) 9408        input_image[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_35 (ReLU)                 (None, 256, 256, 64) 0           encoder_block_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_block_2 (Conv2D)        (None, 128, 128, 128 73728       re_lu_35[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_21 (Inst (None, 128, 128, 128 256         encoder_block_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_36 (ReLU)                 (None, 128, 128, 128 0           instance_normalization_21[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "encoder_block_3 (Conv2D)        (None, 64, 64, 256)  294912      re_lu_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_22 (Inst (None, 64, 64, 256)  512         encoder_block_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_37 (ReLU)                 (None, 64, 64, 256)  0           instance_normalization_22[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_1_1 (Conv2D)  (None, 64, 64, 256)  589824      re_lu_37[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_38 (ReLU)                 (None, 64, 64, 256)  0           transformer_block_1_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_1_2 (Conv2D)  (None, 64, 64, 256)  589824      re_lu_38[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 64, 64, 256)  0           transformer_block_1_2[0][0]      \n",
            "                                                                 re_lu_37[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_2_1 (Conv2D)  (None, 64, 64, 256)  589824      add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_39 (ReLU)                 (None, 64, 64, 256)  0           transformer_block_2_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_2_2 (Conv2D)  (None, 64, 64, 256)  589824      re_lu_39[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 64, 64, 256)  0           transformer_block_2_2[0][0]      \n",
            "                                                                 add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_3_1 (Conv2D)  (None, 64, 64, 256)  589824      add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_40 (ReLU)                 (None, 64, 64, 256)  0           transformer_block_3_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_3_2 (Conv2D)  (None, 64, 64, 256)  589824      re_lu_40[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 64, 64, 256)  0           transformer_block_3_2[0][0]      \n",
            "                                                                 add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_4_1 (Conv2D)  (None, 64, 64, 256)  589824      add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_41 (ReLU)                 (None, 64, 64, 256)  0           transformer_block_4_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_4_2 (Conv2D)  (None, 64, 64, 256)  589824      re_lu_41[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 64, 64, 256)  0           transformer_block_4_2[0][0]      \n",
            "                                                                 add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_5_1 (Conv2D)  (None, 64, 64, 256)  589824      add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_42 (ReLU)                 (None, 64, 64, 256)  0           transformer_block_5_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_5_2 (Conv2D)  (None, 64, 64, 256)  589824      re_lu_42[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 64, 64, 256)  0           transformer_block_5_2[0][0]      \n",
            "                                                                 add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_6_1 (Conv2D)  (None, 64, 64, 256)  589824      add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_43 (ReLU)                 (None, 64, 64, 256)  0           transformer_block_6_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_6_2 (Conv2D)  (None, 64, 64, 256)  589824      re_lu_43[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 64, 64, 256)  0           transformer_block_6_2[0][0]      \n",
            "                                                                 add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "enc_dec_skip_1 (Concatenate)    (None, 64, 64, 512)  0           add_23[0][0]                     \n",
            "                                                                 re_lu_37[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "decoder_block_1 (Conv2DTranspos (None, 128, 128, 128 589824      enc_dec_skip_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_23 (Inst (None, 128, 128, 128 256         decoder_block_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_44 (ReLU)                 (None, 128, 128, 128 0           instance_normalization_23[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "enc_dec_skip_2 (Concatenate)    (None, 128, 128, 256 0           re_lu_44[0][0]                   \n",
            "                                                                 re_lu_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "decoder_block_2 (Conv2DTranspos (None, 256, 256, 64) 147456      enc_dec_skip_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_24 (Inst (None, 256, 256, 64) 128         decoder_block_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_45 (ReLU)                 (None, 256, 256, 64) 0           instance_normalization_24[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "enc_dec_skip_3 (Concatenate)    (None, 256, 256, 128 0           re_lu_45[0][0]                   \n",
            "                                                                 re_lu_35[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output_block (Conv2D)   (None, 256, 256, 3)  18816       enc_dec_skip_3[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 8,213,184\n",
            "Trainable params: 8,213,184\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rstz5asS_d2g"
      },
      "source": [
        "**Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVBf2uEu_gZe",
        "outputId": "14382d16-e9ec-4a2e-98c6-3583ffb8721b"
      },
      "source": [
        "def discriminator_fn(height=HEIGHT, width=WIDTH, channels=CHANNELS):\n",
        "    inputs = L.Input(shape=[height, width, channels], name='input_image')\n",
        "    # inputs_patch = L.experimental.preprocessing.RandomCrop(height=70, width=70, name='input_image_patch')(inputs) # Works only with GPU\n",
        "\n",
        "    # Encoder    \n",
        "    x = encoder_block(inputs, 64,  4, 2, apply_instancenorm=False, activation=L.LeakyReLU(0.2), name='block_1') # (bs, 128, 128, 64)\n",
        "    x = encoder_block(x, 128, 4, 2, apply_instancenorm=True, activation=L.LeakyReLU(0.2), name='block_2')       # (bs, 64, 64, 128)\n",
        "    x = encoder_block(x, 256, 4, 2, apply_instancenorm=True, activation=L.LeakyReLU(0.2), name='block_3')       # (bs, 32, 32, 256)\n",
        "    x = encoder_block(x, 512, 4, 1, apply_instancenorm=True, activation=L.LeakyReLU(0.2), name='block_4')       # (bs, 32, 32, 512)\n",
        "\n",
        "    # TODO testear con una capa extra, modelo de pinturas da mejor resultado con 5 layers en el discriminator\n",
        "    outputs = L.Conv2D(1, 4, strides=1, padding='valid', kernel_initializer=conv_initializer)(x)                # (bs, 29, 29, 1)\n",
        "    \n",
        "    discriminator = Model(inputs, outputs)\n",
        "    \n",
        "    return discriminator\n",
        "\n",
        "\n",
        "sample_discriminator = discriminator_fn()\n",
        "sample_discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_image (InputLayer)     [(None, 256, 256, 3)]     0         \n",
            "_________________________________________________________________\n",
            "encoder_block_1 (Conv2D)     (None, 128, 128, 64)      3072      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)   (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "encoder_block_2 (Conv2D)     (None, 64, 64, 128)       131072    \n",
            "_________________________________________________________________\n",
            "instance_normalization_25 (I (None, 64, 64, 128)       256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "encoder_block_3 (Conv2D)     (None, 32, 32, 256)       524288    \n",
            "_________________________________________________________________\n",
            "instance_normalization_26 (I (None, 32, 32, 256)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "encoder_block_4 (Conv2D)     (None, 32, 32, 512)       2097152   \n",
            "_________________________________________________________________\n",
            "instance_normalization_27 (I (None, 32, 32, 512)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 32, 32, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 29, 29, 1)         8193      \n",
            "=================================================================\n",
            "Total params: 2,765,569\n",
            "Trainable params: 2,765,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y30ilbl3_oo6"
      },
      "source": [
        "**Build CycleGAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHyrU43k_sCl"
      },
      "source": [
        "with strategy.scope():\n",
        "    vintage_generator = generator_fn(height=None, width=None, transformer_blocks=TRANSFORMER_BLOCKS) # transforms photos to vintage photos\n",
        "    photo_generator = generator_fn(height=None, width=None, transformer_blocks=TRANSFORMER_BLOCKS) # transforms vintage photos to be more like photos\n",
        "\n",
        "    vintage_discriminator = discriminator_fn(height=None, width=None) # differentiates real vintage photos and generated vintage photos\n",
        "    photo_discriminator = discriminator_fn(height=None, width=None) # differentiates real photos and generated photos\n",
        "\n",
        "\n",
        "class CycleGan(Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vintage_generator,\n",
        "        photo_generator,\n",
        "        vintage_discriminator,\n",
        "        photo_discriminator,\n",
        "        lambda_cycle=10,\n",
        "    ):\n",
        "        super(CycleGan, self).__init__()\n",
        "        self.v_gen = vintage_generator\n",
        "        self.p_gen = photo_generator\n",
        "        self.v_disc = vintage_discriminator\n",
        "        self.p_disc = photo_discriminator\n",
        "        self.lambda_cycle = lambda_cycle\n",
        "        \n",
        "    def compile(\n",
        "        self,\n",
        "        v_gen_optimizer,\n",
        "        p_gen_optimizer,\n",
        "        v_disc_optimizer,\n",
        "        p_disc_optimizer,\n",
        "        gen_loss_fn,\n",
        "        disc_loss_fn,\n",
        "        cycle_loss_fn,\n",
        "        identity_loss_fn\n",
        "    ):\n",
        "        super(CycleGan, self).compile()\n",
        "        self.v_gen_optimizer = v_gen_optimizer\n",
        "        self.p_gen_optimizer = p_gen_optimizer\n",
        "        self.v_disc_optimizer = v_disc_optimizer\n",
        "        self.p_disc_optimizer = p_disc_optimizer\n",
        "        self.gen_loss_fn = gen_loss_fn\n",
        "        self.disc_loss_fn = disc_loss_fn\n",
        "        self.cycle_loss_fn = cycle_loss_fn\n",
        "        self.identity_loss_fn = identity_loss_fn\n",
        "        \n",
        "    def train_step(self, batch_data):\n",
        "        real_vintage, real_photo = batch_data\n",
        "        \n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # photo to vintage back to photo\n",
        "            fake_vintage = self.v_gen(real_photo, training=True)\n",
        "            cycled_photo = self.p_gen(fake_vintage, training=True)\n",
        "\n",
        "            # vintage to photo back to vintage\n",
        "            fake_photo = self.p_gen(real_vintage, training=True)\n",
        "            cycled_vintage = self.v_gen(fake_photo, training=True)\n",
        "\n",
        "            # generating itself\n",
        "            same_vintage = self.v_gen(real_vintage, training=True)\n",
        "            same_photo = self.p_gen(real_photo, training=True)\n",
        "\n",
        "            # discriminator used to check, inputing real images\n",
        "            disc_real_vintage = self.v_disc(real_vintage, training=True)\n",
        "            disc_real_photo = self.p_disc(real_photo, training=True)\n",
        "\n",
        "            # discriminator used to check, inputing fake images\n",
        "            disc_fake_vintage = self.v_disc(fake_vintage, training=True)\n",
        "            disc_fake_photo = self.p_disc(fake_photo, training=True)\n",
        "\n",
        "            # evaluates generator loss\n",
        "            vintage_gen_loss = self.gen_loss_fn(disc_fake_vintage)\n",
        "            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n",
        "\n",
        "            # evaluates total cycle consistency loss\n",
        "            total_cycle_loss = self.cycle_loss_fn(real_vintage, cycled_vintage, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n",
        "\n",
        "            # evaluates total generator loss\n",
        "            total_vintage_gen_loss = vintage_gen_loss + total_cycle_loss + self.identity_loss_fn(real_vintage, same_vintage, self.lambda_cycle)\n",
        "            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n",
        "\n",
        "            # evaluates discriminator loss\n",
        "            vintage_disc_loss = self.disc_loss_fn(disc_real_vintage, disc_fake_vintage)\n",
        "            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n",
        "\n",
        "        # Calculate the gradients for generator and discriminator\n",
        "        vintage_generator_gradients = tape.gradient(total_vintage_gen_loss,\n",
        "                                                  self.v_gen.trainable_variables)\n",
        "        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n",
        "                                                  self.p_gen.trainable_variables)\n",
        "\n",
        "        vintage_discriminator_gradients = tape.gradient(vintage_disc_loss,\n",
        "                                                      self.v_disc.trainable_variables)\n",
        "        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n",
        "                                                      self.p_disc.trainable_variables)\n",
        "\n",
        "        # Apply the gradients to the optimizer\n",
        "        self.v_gen_optimizer.apply_gradients(zip(vintage_generator_gradients,\n",
        "                                                 self.v_gen.trainable_variables))\n",
        "\n",
        "        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n",
        "                                                 self.p_gen.trainable_variables))\n",
        "\n",
        "        self.v_disc_optimizer.apply_gradients(zip(vintage_discriminator_gradients,\n",
        "                                                  self.v_disc.trainable_variables))\n",
        "\n",
        "        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n",
        "                                                  self.p_disc.trainable_variables))\n",
        "        \n",
        "        return {'vintage_gen_loss': total_vintage_gen_loss,\n",
        "                'photo_gen_loss': total_photo_gen_loss,\n",
        "                'vintage_disc_loss': vintage_disc_loss,\n",
        "                'photo_disc_loss': photo_disc_loss\n",
        "               }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tM8nD2wAuQf"
      },
      "source": [
        "**Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwV28mrAAw8-"
      },
      "source": [
        "with strategy.scope():\n",
        "    # Discriminator loss {0: fake, 1: real} (The discriminator loss outputs the average of the real and generated loss)\n",
        "    def discriminator_loss(real, generated):\n",
        "        real_loss = losses.BinaryCrossentropy(from_logits=True, reduction=losses.Reduction.NONE)(tf.ones_like(real), real)\n",
        "\n",
        "        generated_loss = losses.BinaryCrossentropy(from_logits=True, reduction=losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n",
        "\n",
        "        total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "        return total_disc_loss * 0.5\n",
        "    \n",
        "    # Generator loss\n",
        "    def generator_loss(generated):\n",
        "        return losses.BinaryCrossentropy(from_logits=True, reduction=losses.Reduction.NONE)(tf.ones_like(generated), generated)\n",
        "    \n",
        "    \n",
        "    # Cycle consistency loss (measures if original photo and the twice transformed photo to be similar to one another)\n",
        "    with strategy.scope():\n",
        "        def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n",
        "          #TODO change here\n",
        "            # loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
        "            # print(type(real_image))\n",
        "            # print(real_image)\n",
        "            loss1 = tf.reduce_mean(tf.image.ssim(real_image, cycled_image, 256))\n",
        "\n",
        "            return LAMBDA * loss1\n",
        "\n",
        "    # Identity loss (compares the image with its generator (i.e. photo with photo generator))\n",
        "    with strategy.scope():\n",
        "        def identity_loss(real_image, same_image, LAMBDA):\n",
        "            loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
        "            return LAMBDA * 0.5 * loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm32CnKUA5EU"
      },
      "source": [
        "**Learning rate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "KKzRPdkEA8Nb",
        "outputId": "654095a5-ed54-4fbd-c780-2f8dae2bdcfd"
      },
      "source": [
        "@tf.function\n",
        "def linear_schedule_with_warmup(step):\n",
        "    \"\"\" Create a schedule with a learning rate that decreases linearly after\n",
        "    linearly increasing during a warmup period.\n",
        "    \"\"\"\n",
        "    lr_start   = 2e-4\n",
        "    lr_max     = 2e-4\n",
        "    lr_min     = 0.\n",
        "    \n",
        "    steps_per_epoch = int(max(n_vintage_samples, n_photo_samples)//BATCH_SIZE)\n",
        "    total_steps = EPOCHS * steps_per_epoch\n",
        "    warmup_steps = 1\n",
        "    hold_max_steps = total_steps * 0.5 ## 100 epochs at consistent rate, then decreases to 0\n",
        "    \n",
        "    if step < warmup_steps:\n",
        "        lr = (lr_max - lr_start) / warmup_steps * step + lr_start\n",
        "    elif step < warmup_steps + hold_max_steps:\n",
        "        lr = lr_max\n",
        "    else:\n",
        "        lr = lr_max * ((total_steps - step) / (total_steps - warmup_steps - hold_max_steps))\n",
        "        if lr_min is not None:\n",
        "            lr = tf.math.maximum(lr_min, lr)\n",
        "\n",
        "    return lr\n",
        "\n",
        "steps_per_epoch = int(max(n_vintage_samples, n_photo_samples)//BATCH_SIZE)\n",
        "total_steps = EPOCHS * steps_per_epoch\n",
        "rng = [i for i in range(0, total_steps, 50)]\n",
        "y = [linear_schedule_with_warmup(x) for x in rng]\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "fig, ax = plt.subplots(figsize=(20, 6))\n",
        "plt.plot(rng, y)\n",
        "print(f'{EPOCHS} total epochs and {steps_per_epoch} steps per epoch')\n",
        "print(f'Learning rate schedule: {y[0]:.3g} to {max(y):.3g} to {y[-1]:.3g}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function linear_schedule_with_warmup at 0x7fa674840710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function linear_schedule_with_warmup at 0x7fa674840710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function linear_schedule_with_warmup at 0x7fa674840710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function linear_schedule_with_warmup at 0x7fa674840710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 total epochs and 25 steps per epoch\n",
            "Learning rate schedule: 0.0002 to 0.0002 to 4e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x432 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abNd5bZCBCSh"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc5UzrpJBDxf"
      },
      "source": [
        "with strategy.scope():\n",
        "    # Create generators\n",
        "    lr_vintage_gen = lambda: linear_schedule_with_warmup(tf.cast(vintage_generator_optimizer.iterations, tf.float32))\n",
        "    lr_photo_gen = lambda: linear_schedule_with_warmup(tf.cast(photo_generator_optimizer.iterations, tf.float32))\n",
        "    \n",
        "    # TODO: Change Adam to AdamW optimizer\n",
        "    vintage_generator_optimizer = optimizers.Adam(learning_rate=lr_vintage_gen, beta_1=0.5)\n",
        "    photo_generator_optimizer = optimizers.Adam(learning_rate=lr_photo_gen, beta_1=0.5)\n",
        "\n",
        "    # Create discriminators\n",
        "    lr_vintage_disc = lambda: linear_schedule_with_warmup(tf.cast(vintage_discriminator_optimizer.iterations, tf.float32))\n",
        "    lr_photo_disc = lambda: linear_schedule_with_warmup(tf.cast(photo_discriminator_optimizer.iterations, tf.float32))\n",
        "    \n",
        "    # TODO: Change Adam to AdamW optimizer\n",
        "    vintage_discriminator_optimizer = optimizers.Adam(learning_rate=lr_vintage_disc, beta_1=0.5)\n",
        "    photo_discriminator_optimizer = optimizers.Adam(learning_rate=lr_photo_disc, beta_1=0.5)\n",
        "\n",
        "    \n",
        "    # Create GAN\n",
        "    gan_model = CycleGan(vintage_generator, photo_generator, \n",
        "                         vintage_discriminator, photo_discriminator)\n",
        "\n",
        "    gan_model.compile(v_gen_optimizer=vintage_generator_optimizer,\n",
        "                      p_gen_optimizer=photo_generator_optimizer,\n",
        "                      v_disc_optimizer=vintage_discriminator_optimizer,\n",
        "                      p_disc_optimizer=photo_discriminator_optimizer,\n",
        "                      gen_loss_fn=generator_loss,\n",
        "                      disc_loss_fn=discriminator_loss,\n",
        "                      cycle_loss_fn=calc_cycle_loss,\n",
        "                      identity_loss_fn=identity_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaJ87eYKBSH1"
      },
      "source": [
        "# Create dataset\n",
        "vintage_ds = get_dataset(VINTAGE_FILENAMES, batch_size=BATCH_SIZE)\n",
        "photo_ds = get_dataset(PHOTO_FILENAMES, batch_size=BATCH_SIZE)\n",
        "gan_ds = tf.data.Dataset.zip((vintage_ds, photo_ds))\n",
        "\n",
        "photo_ds_eval = get_dataset(PHOTO_FILENAMES, repeat=False, shuffle=False, batch_size=1)\n",
        "vintage_ds_eval = get_dataset(VINTAGE_FILENAMES, repeat=False, shuffle=False, batch_size=1)\n",
        "\n",
        "# Callbacks\n",
        "class GANMonitor(Callback):\n",
        "    \"\"\"A callback to generate and save images after each epoch\"\"\"\n",
        "\n",
        "    def __init__(self, num_img=5, vintage_path='/content/drive/MyDrive/DeepLearningTPs/collab/vintage_out_SSIM_batch_55', photo_path='/content/drive/MyDrive/DeepLearningTPs/collab/photo_out_SSIM_batch_55'):\n",
        "        self.num_img = num_img\n",
        "        self.vintage_path = vintage_path\n",
        "        self.photo_path = photo_path\n",
        "        # Create directories to save the generate images\n",
        "        if not os.path.exists(self.vintage_path):\n",
        "            os.makedirs(self.vintage_path)\n",
        "        if not os.path.exists(self.photo_path):\n",
        "            os.makedirs(self.photo_path)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # vintage generated images\n",
        "        for i, img in enumerate(photo_ds_eval.take(self.num_img)):\n",
        "            prediction = vintage_generator(img, training=False)[0].numpy()\n",
        "            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "            prediction = PIL.Image.fromarray(prediction)\n",
        "            prediction.save(f'{self.vintage_path}/generated_{i}_{epoch+1}.png')\n",
        "            \n",
        "        # Photo generated images\n",
        "        for i, img in enumerate(vintage_ds_eval.take(self.num_img)):\n",
        "            prediction = photo_generator(img, training=False)[0].numpy()\n",
        "            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "            prediction = PIL.Image.fromarray(prediction)\n",
        "            prediction.save(f'{self.photo_path}/generated_{i}_{epoch+1}.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fwhroemf6VP"
      },
      "source": [
        "!pip install -q pyyaml h5py  # Required to save models in HDF5 format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv_qixf-f7ew"
      },
      "source": [
        "filepath = '/content/drive/MyDrive/DeepLearningTPs/collab/model_progress'\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath= filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GJ7vJUFFBkpe",
        "outputId": "5e39dca4-6d81-427f-8a5c-22ffd4d43b27"
      },
      "source": [
        "gan_model.fit(gan_ds, epochs=EPOCHS,\n",
        "              callbacks=[GANMonitor()],\n",
        "              steps_per_epoch=(max(n_vintage_samples, n_photo_samples)//BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-42eb7876fe51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m gan_model.fit(gan_ds, epochs=EPOCHS,\n\u001b[1;32m      2\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGANMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m               steps_per_epoch=(max(n_vintage_samples, n_photo_samples)//BATCH_SIZE))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1187\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \"\"\"\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    510\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \"\"\"\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1060\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: 9 root error(s) found.\n  (0) Resource exhausted: {{function_node __inference_train_function_183867}} Attempting to allocate 387.42M. That was not possible. There are 338.73M free. Due to fragmentation, the largest contiguous region of free memory is 323.48M.; (1x0x0_HBM0)\n\t [[{{node cluster_train_function/_execute_2_0}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) Resource exhausted: {{function_node __inference_train_function_183867}} Attempting to allocate 387.42M. That was not possible. There are 338.73M free. Due to fragmentation, the largest contiguous region of free memory is 323.48M.; (0x1x0_HBM0)\n\t [[{{node cluster_train_function/_execute_4_0}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (2) Resource exhausted: {{function_node __inference_train_function_183867}} Attempting to allocate 387.42M. That was not possible. There are 338.73M free. Due to fragmentation, the largest contiguous region of free memory is 323.48M.; (1x1x0_HBM0)\n\t [[{{node cluster_train_function/_execute_6_0}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (3) Resource exhausted: {{function_node __inference_train_function_183867}} Attempting to allocate 387.42M. That was not possible. There are 353.73M free. Due to fragmentation, the largest contiguous region of free memory is 323.48M.; (1x1x0_HBM1)\n\t [[{{node cluster_train_function/_execute_7_0}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (4) Cancelled: {{function_node __inference_train_function_183867}} RPC cancelled, not running TPU program on device 5\n\t [[{{node cluster_train_function/_execute_5_0}}]]\n  (5) Cancelled: {{function_node __inference_train_function_183867}} RPC cancelled, not running TPU program on device 3\n\t [[{{node cluster_train_function/_execute_3_0}}]]\n  (6) Cancelled: {{function_node __inference_train_function_183867}} RPC cancelled, not running TPU program on device 0\n\t [[{{node cluster_train_function/_execute_0_0}}]]\n  (7) Resource exhausted: {{function_node __inference_train_function_183867}} Attempting to allocate 387.42M. That was not possible. There are 338.73M free. Due to fragmentation, the largest contiguous region of free memory is 323.48M.; (0x0x0_HBM1)\n\t [[{{node cluster_train_function/_execute_1_0}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[cluster_trai ... [truncated]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfKYD7h5-ZoM"
      },
      "source": [
        "create_gif('/content/drive/MyDrive/DeepLearningTPs/collab/vintage_out_reg_batch_60/*.png', '/content/drive/MyDrive/DeepLearningTPs/collab/vintage_gif_reg_batch_60.gif') # Create vintage gif"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7WH7TA9BBcv"
      },
      "source": [
        "create_gif('/content/drive/MyDrive/DeepLearningTPs/collab/photo_out_reg_batch_60/*.png', '/content/drive/MyDrive/DeepLearningTPs/collab/photo_gif_reg_batch_60.gif') # Create vintage gif"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "h0D1S9pXAQcP",
        "outputId": "1f550e0f-aa1f-4a96-f128-e30e741af913"
      },
      "source": [
        "import sys\n",
        "import imageio\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/DeepLearningTPs/collab/facenetmaster/src')\n",
        "\n",
        "from compare import compare_imgs\n",
        "\n",
        "model = \"/content/drive/MyDrive/DeepLearningTPs/collab/20180402-114759\" \n",
        "face_1 = imageio.imread(\"/content/drive/MyDrive/DeepLearningTPs/dataset/test_face_recog_1.jpg\") \n",
        "face_2 = imageio.imread(\"/content/drive/MyDrive/DeepLearningTPs/dataset/test_face_recog_2.jpg\") \n",
        "\n",
        "print(compare_imgs(model, face_1, face_2)[1])\n",
        "\n",
        "# loss1 = tf.math.squared_difference(new_model.predict(real_image), new_model.predict(cycled_image))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/argparse.py:1829: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if arg_string == '--':\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-cb68342baf7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mface_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/DeepLearningTPs/dataset/test_face_recog_2.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompare_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# !python /content/drive/MyDrive/DeepLearningTPs/collab/facenet-master/src/compare.py /content/drive/MyDrive/DeepLearningTPs/collab/20180402-114759 \"/content/drive/MyDrive/DeepLearningTPs/dataset/test_face_recog_1.jpg\" \"/content/drive/MyDrive/DeepLearningTPs/dataset/test_face_recog_2.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# subprocess = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/DeepLearningTPs/collab/facenetmaster/src/compare.py\u001b[0m in \u001b[0;36mcompare_imgs\u001b[0;34m(model, img_1, img_2)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompare_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/DeepLearningTPs/collab/facenetmaster/src/compare.py\u001b[0m in \u001b[0;36mparse_arguments\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    130\u001b[0m     parser.add_argument('--gpu_memory_fraction', type=float,\n\u001b[1;32m    131\u001b[0m         help='Upper bound on the amount of GPU memory that will be used by the process.', default=1.0)\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1762\u001b[0m     \u001b[0;31m# =====================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1764\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mparse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1794\u001b[0m         \u001b[0;31m# parse the arguments and exit if there are any errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1796\u001b[0;31m             \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1797\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_UNRECOGNIZED_ARGS_ATTR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_UNRECOGNIZED_ARGS_ATTR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36m_parse_known_args\u001b[0;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[1;32m   1835\u001b[0m             \u001b[0;31m# and note the index if it was an option\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1837\u001b[0;31m                 \u001b[0moption_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1838\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moption_tuple\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m                     \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'A'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36m_parse_optional\u001b[0;34m(self, arg_string)\u001b[0m\n\u001b[1;32m   2118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parse_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m         \u001b[0;31m# if it's an empty string, it was meant to be a positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2120\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marg_string\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmBjdEZA6sTx"
      },
      "source": [
        "import sys\n",
        "import PIL\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/DeepLearningTPs/collab/facenetmaster/src')\n",
        "\n",
        "from compare import compare_imgs\n",
        "\n",
        "with strategy.scope():\n",
        "    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n",
        "        # loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
        "        loss1 = tf.reduce_mean(tf.image.ssim(real_image, cycled_image, 256))\n",
        "        # model = \"/content/drive/MyDrive/DeepLearningTPs/collab/20180402-114759\"\n",
        "        # real_image_path = '/content/drive/MyDrive/DeepLearningTPs/collab/img_1.jpg'\n",
        "        # cycled_image_path = '/content/drive/MyDrive/DeepLearningTPs/collab/img_2.jpg'\n",
        "\n",
        "        # loss1 = tf.reduce_mean(compare_imgs(model, real_image_path, cycled_image_path)[1])\n",
        "\n",
        "        return LAMBDA * loss1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JLTef-aOGEA"
      },
      "source": [
        "!pip install -q pyyaml h5py  # Required to save models in HDF5 format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "gGhwtZyb6yeb",
        "outputId": "655c6162-937e-45d7-c9b1-634094451b55"
      },
      "source": [
        "import os\n",
        "\n",
        "filepath = '/content/drive/MyDrive/DeepLearningTPs/collab'\n",
        "metric = 'val_photo_disc_loss'\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath= filepath, monitor=metric)\n",
        "vintage_ds = get_dataset(VINTAGE_PHOTOS_FILENAMES, batch_size=16)\n",
        "photo_ds = get_dataset(REAL_PHOTOS_FILENAMES, batch_size=16)\n",
        "gan_ds = tf.data.Dataset.zip((vintage_ds, photo_ds))\n",
        "# cycle_gan_model.fit(\n",
        "#     ),\n",
        "#     epochs=10,\n",
        "#     callbacks= [checkpoint_callback]\n",
        "# )\n",
        "\n",
        "cycle_gan_model.fit(gan_ds, epochs=100, \n",
        "                        steps_per_epoch=(max(len(VINTAGE_PHOTOS_FILENAMES), len(REAL_PHOTOS_FILENAMES))//16))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "ename": "DataLossError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-33d76e40bdbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m cycle_gan_model.fit(gan_ds, epochs=100, \n\u001b[0;32m---> 16\u001b[0;31m                         steps_per_epoch=(max(len(VINTAGE_PHOTOS_FILENAMES), len(REAL_PHOTOS_FILENAMES))//16))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDataLossError\u001b[0m: 2 root error(s) found.\n  (0) Data loss:  corrupted record at 0\n\t [[node IteratorGetNext (defined at <ipython-input-27-33d76e40bdbb>:16) ]]\n  (1) Data loss:  corrupted record at 0\n\t [[node IteratorGetNext (defined at <ipython-input-27-33d76e40bdbb>:16) ]]\n\t [[IteratorGetNext/_2]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_50783]\n\nFunction call stack:\ntrain_function -> train_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "zNsAawpN60t0",
        "outputId": "05f9b8ec-fb9e-4bf3-bd20-38e4acb10e6f"
      },
      "source": [
        "cycle_gan_model.summary()\n",
        "_, ax = plt.subplots(5, 2, figsize=(12, 12))\n",
        "for i, img in enumerate(real_photo_ds.take(5)):\n",
        "    prediction = vintage_generator(img, training=False)[0].numpy()\n",
        "    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
        "\n",
        "    ax[i, 0].imshow(img)\n",
        "    ax[i, 1].imshow(prediction)\n",
        "    ax[i, 0].set_title(\"Input Photo\")\n",
        "    ax[i, 1].set_title(\"Vintage-esque\")\n",
        "    ax[i, 0].axis(\"off\")\n",
        "    ax[i, 1].axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-f25f1ca44ad3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcycle_gan_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_photo_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvintage_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m127.5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m127.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   2519\u001b[0m     \"\"\"\n\u001b[1;32m   2520\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2521\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   2522\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ7gAXhjRG4S"
      },
      "source": [
        "this_thing = False\n",
        "if this_thing:\n",
        "  #no need to run twice if the files have already been created\n",
        "  from PIL import Image\n",
        "  import os, sys\n",
        "\n",
        "  #path = \"/content/drive/MyDrive/DeepLearningTPs/dataset/training_real/\"\n",
        "  path = \"/content/drive/MyDrive/DeepLearningTPs/dataset/vintage_portrait/\"\n",
        "  #save_path = \"/content/drive/MyDrive/DeepLearningTPs/dataset/training_real_resized/\"\n",
        "  save_path = \"/content/drive/MyDrive/DeepLearningTPs/dataset/vintage_portrait_resized/\"\n",
        "  dirs = os.listdir( path )\n",
        "\n",
        "  def resize():\n",
        "      for item in dirs:\n",
        "          if os.path.isfile(path+item):\n",
        "            try: \n",
        "              im = Image.open(path+item)\n",
        "              f, e = os.path.splitext(save_path+item)\n",
        "              imResize = im.resize((256,256), Image.ANTIALIAS)\n",
        "              imResize.save(f + 'resized.jpg', 'JPEG', quality=90)\n",
        "            except:\n",
        "              print('error', path+item)\n",
        "  resize()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}